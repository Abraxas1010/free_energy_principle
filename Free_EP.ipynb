{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "Standard optimization algorithms, such as gradient descent and evolutionary methods, are often constrained by their susceptibility to local optima. To address this, we investigate Active Inference, a first-principles framework derived from the Free Energy Principle, as a more robust method for state-space search and decision-making under uncertainty. We implement a computational agent in a generic, partially observable grid-world environment, where action selection is driven by the imperative to minimize expected free energy. Initial results demonstrate that the agent exhibits a clear preference for epistemic actions that resolve environmental uncertainty, even at the temporary cost of immediate progress toward a specified goal. This emergent information-seeking behavior suggests that Active Inference provides a powerful mechanism for developing autonomous systems that can effectively balance exploration and exploitation, thereby enabling more sophisticated and efficient navigation of complex problem landscapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Beyond Local Optima\n",
    "\n",
    "A fundamental challenge in artificial intelligence and machine learning is designing agents that can navigate complex and uncertain environments to achieve their goals. Traditional optimization techniques, from gradient descent to evolutionary algorithms, provide powerful tools but often fail when the problem landscape contains numerous local optima, trapping the agent in suboptimal solutions.\n",
    "\n",
    "The Free Energy Principle (FEP) offers a comprehensive, first-principles approach to understanding intelligent behavior, viewing agents not as simple optimizers but as systems that actively try to make sense of their world. Agents driven by the FEP, a process known as Active Inference, act to minimize their long-term surprise. This single imperative unifies action and perception, driving the agent to both update its internal model of the world (its beliefs) and act upon the world to make it conform to that model. A key consequence is that actions are selected not just to achieve goals, but also to reduce uncertainty, enabling a natural and sophisticated balance between exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Environment: A Partially Observable World\n",
    "\n",
    "To test this framework, we construct a generic grid-world environment. This serves as a controlled laboratory for analyzing agent behavior. The world is partially observable; the agent knows its own location but does not have a priori knowledge of the location of obstacles. It must infer the layout of the world through exploration.\n",
    "\n",
    "The agent's objective is to navigate from a starting position to a goal position while avoiding obstacles. This simple setup is sufficient to demonstrate the core principles of Active Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# -- Environment Configuration --\n",
    "GRID_DIMENSIONS = (5, 5)\n",
    "START_POSITION = (0, 0)\n",
    "GOAL_POSITION = (4, 4)\n",
    "OBSTACLE_POSITIONS = [(2, 1), (3, 3)]\n",
    "\n",
    "# -- Simulation Parameters --\n",
    "MAX_STEPS = 50\n",
    "EXPLORATION_RATE = 0.05 # Probability of taking a random action\n",
    "\n",
    "def setup_environment(dimensions, obstacles):\n",
    "    \"\"\"Initializes the grid world environment with obstacles.\"\"\"\n",
    "    grid_world = np.zeros(dimensions, dtype=int)\n",
    "    for obs_pos in obstacles:\n",
    "        grid_world[obs_pos] = 1 # 1 represents an obstacle\n",
    "    return grid_world\n",
    "\n",
    "# Initialize the global environment\n",
    "grid_world = setup_environment(GRID_DIMENSIONS, OBSTACLE_POSITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Active Inference Agent\n",
    "\n",
    "Our agent is designed according to the principles of Active Inference. Its behavior is governed by an **internal generative model**, which represents its beliefs about how the environment works. This model includes:\n",
    "\n",
    "1.  **Beliefs about States ($p(s)$):** A probability distribution over all possible locations in the grid, representing the agent's confidence about where it is and where obstacles might be.\n",
    "2.  **A Model of Action ($p(s'|s, a)$):** The agent's understanding of how actions ($a$) cause transitions between states ($s$ to $s'$).\n",
    "\n",
    "Action and perception are two sides of the same coin:\n",
    "-   **Perception:** When the agent receives sensory information (e.g., it bumps into an obstacle), it updates its beliefs to better match the reality of the world. This is a process of Bayesian inference.\n",
    "-   **Action:** The agent chooses actions that are expected to minimize its free energy. This **Expected Free Energy (EFE)** has two components that the agent seeks to optimize simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Implementing Expected Free Energy\n",
    "\n",
    "An agent selects the policy (action) that it believes will minimize its future free energy. This can be decomposed into two key components:\n",
    "\n",
    "$$ \\text{EFE}(\\pi) = \\underbrace{\\mathbb{E}_{p(o,s|\\pi)}[\\ln p(s|o) - \\ln p(s)]}_\\text{Epistemic Value} + \\underbrace{\\mathbb{E}_{p(o,s|\\pi)}[\\ln p(o)]}_\\text{Instrumental Value} $$\n",
    "\n",
    "In simpler terms:\n",
    "\n",
    "* **Instrumental Value:** The drive to reach preferred states. For our agent, this means reaching the goal location. Actions that are likely to lead to the goal have high instrumental value.\n",
    "* **Epistemic Value:** The drive to gather information and reduce uncertainty. Actions that are likely to reveal the true layout of the environment (e.g., discovering the location of an obstacle) have high epistemic value.\n",
    "\n",
    "The agent's genius lies in balancing these two imperatives. It will not rush towards the goal if it is highly uncertain about the path ahead; instead, it will first take actions to explore and reduce that uncertainty, demonstrating a sophisticated and curious form of behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveInferenceAgent:\n",
    "    \"\"\"An agent that uses Active Inference to navigate a grid world.\n",
    "    \n",
    "    The agent maintains a belief distribution over the states of the world and selects\n",
    "    actions that minimize its Expected Free Energy (EFE).\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_dims, start_pos, goal_pos, grid_world):\n",
    "        \"\"\"Initializes the agent.\n",
    "\n",
    "        Args:\n",
    "            grid_dims (tuple): The (height, width) of the grid.\n",
    "            start_pos (tuple): The (y, x) starting coordinates.\n",
    "            goal_pos (tuple): The (y, x) goal coordinates.\n",
    "            grid_world (np.ndarray): The environment layout.\n",
    "        \"\"\"\n",
    "        self.grid_dims = grid_dims\n",
    "        self.position = start_pos\n",
    "        self.goal_pos = goal_pos\n",
    "        self.grid_world = grid_world\n",
    "        \n",
    "        # Beliefs are initialized to be uniform, excluding known start position.\n",
    "        self.beliefs = np.ones(grid_dims) \n",
    "        self.beliefs[self.position] = 1.0 # Agent knows where it starts\n",
    "        self.beliefs = self._normalize_beliefs(self.beliefs)\n",
    "        \n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.history = [start_pos]\n",
    "\n",
    "    def _normalize_beliefs(self, beliefs):\n",
    "        \"\"\"Normalizes a belief distribution to sum to 1.\"\"\"\n",
    "        return beliefs / (np.sum(beliefs) + 1e-8) # Add epsilon to avoid division by zero\n",
    "\n",
    "    def _get_potential_next_state(self, position, action):\n",
    "        \"\"\"Calculates the resulting position from an action.\"\"\"\n",
    "        y, x = position\n",
    "        if action == \"up\": y -= 1\n",
    "        elif action == \"down\": y += 1\n",
    "        elif action == \"left\": x -= 1\n",
    "        elif action == \"right\": x += 1\n",
    "        return (y, x)\n",
    "    \n",
    "    def update_beliefs(self, observation, new_pos):\n",
    "        \"\"\"Updates beliefs based on an observation (Bayesian perception).\"\"\"\n",
    "        # If an obstacle is observed, the belief in that location being an obstacle increases.\n",
    "        # For this simplified model, we directly update beliefs about free space.\n",
    "        if observation == 'obstacle':\n",
    "            self.beliefs[new_pos] = 0.0 # Obstacle confirmed at that position\n",
    "        else:\n",
    "            # If move is successful, we are certain of our new position.\n",
    "            # This is a strong simplifying assumption (proprioceptive certainty).\n",
    "            likelihood = np.zeros(self.grid_dims)\n",
    "            likelihood[new_pos] = 1.0\n",
    "            self.beliefs *= likelihood\n",
    "        \n",
    "        self.beliefs = self._normalize_beliefs(self.beliefs)\n",
    "\n",
    "    def calculate_expected_free_energy(self, action):\n",
    "        \"\"\"Calculates the EFE for a given action.\n",
    "        \n",
    "        EFE is calculated as the negative sum of instrumental and epistemic value.\n",
    "        The agent will choose the action that minimizes this value.\n",
    "        \"\"\"\n",
    "        potential_pos = self._get_potential_next_state(self.position, action)\n",
    "        y, x = potential_pos\n",
    "\n",
    "        # If action leads out of bounds or into a known obstacle, it has infinite EFE (is impossible).\n",
    "        if not (0 <= y < self.grid_dims[0] and 0 <= x < self.grid_dims[1]) or self.beliefs[potential_pos] < 1e-6:\n",
    "            return float('inf')\n",
    "        \n",
    "        # --- Instrumental Value: How much closer does this action get me to my goal? ---\n",
    "        # We use negative log-probability of the goal state under predicted beliefs.\n",
    "        # This is a simple proxy for the preference distribution.\n",
    "        goal_beliefs = np.zeros_like(self.beliefs)\n",
    "        goal_beliefs[self.goal_pos] = 1.0\n",
    "        instrumental_value = np.log(self.beliefs[self.goal_pos] + 1e-8) \n",
    "\n",
    "        # --- Epistemic Value: How much will I learn? ---\n",
    "        # Calculated as the expected information gain (KL divergence between posterior and prior beliefs).\n",
    "        # Simulate the posterior if we move to the potential position.\n",
    "        predicted_posterior = np.zeros_like(self.beliefs)\n",
    "        predicted_posterior[potential_pos] = 1.0 \n",
    "        epistemic_value = entropy(pk=self._normalize_beliefs(predicted_posterior), qk=self.beliefs)\n",
    "        \n",
    "        # We seek to MAXIMIZE value, which is equivalent to MINIMIZING negative value.\n",
    "        return -(instrumental_value + epistemic_value)\n",
    "    \n",
    "    def choose_action(self):\n",
    "        \"\"\"Selects the best action by minimizing EFE, with some randomness.\"\"\"\n",
    "        if np.random.rand() < EXPLORATION_RATE:\n",
    "            return np.random.choice(self.actions)\n",
    "        \n",
    "        efes = {action: self.calculate_expected_free_energy(action) for action in self.actions}\n",
    "        # Select the action with the minimum EFE\n",
    "        best_action = min(efes, key=efes.get)\n",
    "        return best_action\n",
    "\n",
    "    def take_step(self, action):\n",
    "        \"\"\"Executes an action, updating the agent's state and beliefs.\"\"\"\n",
    "        next_pos = self._get_potential_next_state(self.position, action)\n",
    "        y, x = next_pos\n",
    "        \n",
    "        # Check for collision\n",
    "        if not (0 <= y < self.grid_dims[0] and 0 <= x < self.grid_dims[1]) or self.grid_world[next_pos] == 1:\n",
    "            self.update_beliefs(observation='obstacle', new_pos=next_pos)\n",
    "        else:\n",
    "            self.position = next_pos\n",
    "            self.update_beliefs(observation='free', new_pos=next_pos)\n",
    "        self.history.append(self.position)\n",
    "\n",
    "    def visualize_state(self, step):\n",
    "        \"\"\"Visualizes the agent's current state and beliefs.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        # Plot beliefs as a heatmap\n",
    "        belief_map = ax.imshow(self.beliefs, cmap='viridis', interpolation='nearest')\n",
    "        fig.colorbar(belief_map, ax=ax, label='Belief Intensity')\n",
    "        \n",
    "        # Plot grid and obstacles\n",
    "        for r in range(self.grid_dims[0]):\n",
    "            for c in range(self.grid_dims[1]):\n",
    "                if self.grid_world[r, c] == 1:\n",
    "                    ax.add_patch(plt.Rectangle((c-0.5, r-0.5), 1, 1, facecolor='black', edgecolor='black'))\n",
    "\n",
    "        # Plot path history\n",
    "        path = np.array(self.history)\n",
    "        ax.plot(path[:, 1], path[:, 0], marker='o', color='white', alpha=0.5, linestyle='-')\n",
    "\n",
    "        # Plot agent, start, and goal\n",
    "        ax.plot(self.position[1], self.position[0], 'ro', markersize=12, label='Agent')\n",
    "        ax.plot(START_POSITION[1], START_POSITION[0], 'bs', markersize=12, label='Start')\n",
    "        ax.plot(self.goal_pos[1], self.goal_pos[0], 'g*', markersize=15, label='Goal')\n",
    "\n",
    "        ax.set_title(f'Step {step}: Agent State and Beliefs')\n",
    "        ax.set_xticks(np.arange(-.5, self.grid_dims[1], 1), minor=True)\n",
    "        ax.set_yticks(np.arange(-.5, self.grid_dims[0], 1), minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=1)\n",
    "        ax.tick_params(which=\"minor\", size=0)\n",
    "        ax.set_xlim(-0.5, self.grid_dims[1] - 0.5)\n",
    "        ax.set_ylim(self.grid_dims[0] - 0.5, -0.5)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def run_simulation(self, max_steps):\n",
    "        \"\"\"Runs the full simulation loop.\"\"\"\n",
    "        for step in range(max_steps):\n",
    "            self.visualize_state(step)\n",
    "            if self.position == self.goal_pos:\n",
    "                print(f'Goal reached in {step} steps!')\n",
    "                break\n",
    "            \n",
    "            action = self.choose_action()\n",
    "            self.take_step(action)\n",
    "        else:\n",
    "            print(f'Agent did not reach the goal within {max_steps} steps.')\n",
    "        \n",
    "        # Final visualization\n",
    "        print(\"Final state:\")\n",
    "        self.visualize_state(max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulation and Analysis\n",
    "\n",
    "We now instantiate the agent and run the simulation. The following code will execute the agent's perception-action loop. We will visualize the agent's path and its internal belief state at each step.\n",
    "\n",
    "Observe the agent's behavior carefully. A purely goal-driven agent would attempt to move along the diagonal. However, an Active Inference agent will often deviate from the shortest path to first explore its environment. For example, it might move towards a wall to confirm its location, thereby reducing uncertainty. This information-seeking behavior is not explicitly programmed; it emerges directly from the single objective of minimizing expected free energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActiveInferenceAgent(\n",
    "    grid_dims=GRID_DIMENSIONS,\n",
    "    start_pos=START_POSITION,\n",
    "    goal_pos=GOAL_POSITION,\n",
    "    grid_world=grid_world\n",
    ")\n",
    "\n",
    "agent.run_simulation(max_steps=MAX_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Future Work\n",
    "\n",
    "This notebook provides a proof-of-concept demonstrating how Active Inference can produce sophisticated, curious behavior in autonomous agents. By balancing the drive to achieve goals (instrumental value) with the drive to reduce uncertainty (epistemic value), the agent navigates its world more intelligently than one driven by exploitation alone. This is a foundational step, confirming our primary hypothesis.\n",
    "\n",
    "This approach has significant implications for business applications at `apoth3osis` that require robust decision-making in complex environments, from supply chain optimization to autonomous robotics. By avoiding the pitfalls of local optima, these principles can lead to more efficient and resilient AI systems.\n",
    "\n",
    "Future work will involve scaling these principles to more complex, dynamic, and higher-dimensional environments, as well as refining the agent's generative model to incorporate richer forms of sensory data and hierarchical beliefs."
   ]
  }
 ]
}